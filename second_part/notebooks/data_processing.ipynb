{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "## Athlete Data Processing Pipeline\n",
    "\n",
    "This notebook rewrites the data processing portion of our project so that we can interactively test and tweak the transformation steps. In this pipeline, we:\n",
    "- Load athlete data (from local JSON files)\n",
    "- Clean the raw activity data\n",
    "- Extract heart rate zones and build a pace-to-HR regressor\n",
    "- Process activities into weekly and block features\n",
    "- (Optionally) process personal best blocks\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import math\n",
    "import datetime\n",
    "import time\n",
    "import logging\n",
    "from statistics import stdev\n",
    "from scipy import signal, stats\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "# Set up logging\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configuration and Helper Functions\n",
    "\n",
    "Set up file paths and basic helper functions such as loading JSON data,\n",
    "extracting error‑free activities, and retrieving athlete heart rate zones."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_DIR = \"../data\"  # Directory where athlete files are stored\n",
    "\n",
    "def load_json_file(filepath):\n",
    "    try:\n",
    "        with open(filepath, 'r', encoding='utf-8') as f:\n",
    "            return json.load(f)\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error loading {filepath}: {e}\")\n",
    "        return None\n",
    "\n",
    "def get_error_free_activities(activities):\n",
    "    \"\"\"Filter out activities that contain errors.\"\"\"\n",
    "    error_free = []\n",
    "    for act in activities:\n",
    "        if act.get(\"errors\"):\n",
    "            continue\n",
    "        error_free.append(act)\n",
    "    return error_free\n",
    "\n",
    "def get_athlete_zones(athlete_data):\n",
    "    \"\"\"Extract HR zones from athlete data. Falls back to default zones if missing.\"\"\"\n",
    "    try:\n",
    "        zones_raw = athlete_data[\"_Zones\"][\"heart_rate\"][\"zones\"]\n",
    "        return [zones_raw[i]['max'] for i in range(4)]\n",
    "    except Exception:\n",
    "        max_hr = 190\n",
    "        return [round(max_hr * 0.6), round(max_hr * 0.7), round(max_hr * 0.8), round(max_hr * 0.9)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## HR Regressor and Signal Metrics\n",
    "\n",
    "We need a function to build a pace-to-heart‑rate regressor for cases where HR is missing.\n",
    "Also, helper functions to compute signal metrics (standard deviation and frequency of peaks)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_signal_metrics(values):\n",
    "    \"\"\"Calculate standard deviation and frequency of peaks in a signal.\"\"\"\n",
    "    if len(values) < 2:\n",
    "        return None, None\n",
    "    try:\n",
    "        std_val = stdev(values)\n",
    "        peaks = signal.find_peaks(values)[0]\n",
    "        freq = round(len(peaks) / len(values), 2) if len(peaks) > 0 else None\n",
    "        return std_val, freq\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error in calculate_signal_metrics: {e}\")\n",
    "        return None, None\n",
    "\n",
    "def build_pace_to_hr_regressor(activities, athlete_id, zones):\n",
    "    \"\"\"\n",
    "    Build a linear regressor from pace to average heart rate using run activities.\n",
    "    Returns a fitted LinearRegression object and the valid data used.\n",
    "    \"\"\"\n",
    "    data = []\n",
    "    for act in activities:\n",
    "        if act.get(\"type\") == \"Run\":\n",
    "            # Here we assume each run has an average pace (e.g., average_speed) and HR.\n",
    "            hr = act.get(\"average_heartrate\")\n",
    "            pace = act.get(\"average_speed\")\n",
    "            if hr is not None and pace is not None:\n",
    "                data.append({\"athlete_id\": athlete_id, \"mean_hr\": hr, \"pace\": pace})\n",
    "    df = pd.DataFrame(data)\n",
    "    if df.empty or df.shape[0] <= 5:\n",
    "        return None, df\n",
    "    df = df.dropna(subset=[\"mean_hr\", \"pace\"])\n",
    "    X = df[\"pace\"].values.reshape(-1,1)\n",
    "    y = df[\"mean_hr\"].values.reshape(-1,1)\n",
    "    reg = LinearRegression()\n",
    "    reg.fit(X, y)\n",
    "    return reg, df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Activity Processing Functions\n",
    "\n",
    "The following functions mimic your original pipeline but are now rewritten to work in a notebook.\n",
    "\n",
    "- `get_non_run_activity_data`: Extracts basic data from any activity.\n",
    "- `get_run_activity_data`: Processes run-specific data (including HR zones, cadence, elevation changes, and signal metrics).\n",
    "- `extract_activity_features`: Converts a raw activity into a feature row (as a dict) for later DataFrame assembly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_non_run_activity_data(activity, zones):\n",
    "    \"\"\"Extract basic fields from non-run activities.\"\"\"\n",
    "    # Default to activity type 34 (\"Other\") if not found.\n",
    "    activity_types = {\n",
    "        'Ride': 1, 'Run': 2, 'Swim': 3, 'Walk': 4,\n",
    "        # ... other types as needed\n",
    "    }\n",
    "    activity_type = activity_types.get(activity.get(\"type\"), 34)\n",
    "    return {\n",
    "        \"activity_id\": activity.get(\"id\"),\n",
    "        \"activity_type\": activity_type,\n",
    "        \"elapsed_time\": activity.get(\"elapsed_time\"),\n",
    "        \"distance\": activity.get(\"distance\"),\n",
    "        \"mean_hr\": activity.get(\"average_heartrate\")\n",
    "    }\n",
    "\n",
    "def get_run_activity_data(activity, zones, hr_regressor=None):\n",
    "    \"\"\"\n",
    "    Process running activity to extract detailed metrics:\n",
    "      - basic data (id, type, elapsed_time, distance, mean_hr)\n",
    "      - if HR is missing, optionally use hr_regressor to estimate it.\n",
    "      - compute cadence, elevation change, pace, and signal metrics from lap data.\n",
    "    \"\"\"\n",
    "    basic = get_non_run_activity_data(activity, zones)\n",
    "    additional = {}\n",
    "    additional[\"cadence\"] = activity.get(\"average_cadence\")\n",
    "    elev_high = activity.get(\"elev_high\", 0)\n",
    "    elev_low = activity.get(\"elev_low\", 0)\n",
    "    additional[\"elevation\"] = elev_high - elev_low\n",
    "    additional[\"pace\"] = activity.get(\"average_speed\")\n",
    "    # Process lap data if available\n",
    "    lap_data = {\"hr\": [], \"elevation\": [], \"pace\": []}\n",
    "    laps = activity.get(\"laps\", [])\n",
    "    for lap in laps:\n",
    "        hr = lap.get(\"average_heartrate\")\n",
    "        if hr is None and hr_regressor is not None:\n",
    "            # Estimate HR from pace\n",
    "            pace_val = lap.get(\"average_speed\", 0)\n",
    "            hr = hr_regressor.predict([[pace_val]])[0][0]\n",
    "        if hr is not None:\n",
    "            lap_data[\"hr\"].append(hr)\n",
    "        lap_data[\"elevation\"].append(lap.get(\"total_elevation_gain\", 0))\n",
    "        lap_data[\"pace\"].append(lap.get(\"average_speed\", 0))\n",
    "    # Calculate HR signal metrics\n",
    "    hr_std, hr_freq = calculate_signal_metrics(lap_data[\"hr\"]) if lap_data[\"hr\"] else (None, None)\n",
    "    additional[\"hr_std\"] = hr_std\n",
    "    additional[\"hr_freq\"] = hr_freq\n",
    "    # For simplicity, we add similar (but simplified) metrics for elevation and pace\n",
    "    elev_std, elev_freq = calculate_signal_metrics(lap_data[\"elevation\"]) if lap_data[\"elevation\"] else (None, None)\n",
    "    pace_std, pace_freq = calculate_signal_metrics(lap_data[\"pace\"]) if lap_data[\"pace\"] else (None, None)\n",
    "    additional[\"elev_std\"] = elev_std\n",
    "    additional[\"elev_freq\"] = elev_freq\n",
    "    additional[\"pace_std\"] = pace_std\n",
    "    additional[\"pace_freq\"] = pace_freq\n",
    "\n",
    "    # Merge basic and additional data\n",
    "    merged = {**basic, **additional}\n",
    "    return merged\n",
    "\n",
    "def extract_activity_features(activity, zones, hr_regressor=None):\n",
    "    \"\"\"\n",
    "    Determine if an activity is a run or not, and extract corresponding features.\n",
    "    \"\"\"\n",
    "    if activity.get(\"type\") in [\"Run\", \"Trail Run\"]:\n",
    "        features = get_run_activity_data(activity, zones, hr_regressor)\n",
    "    else:\n",
    "        features = get_non_run_activity_data(activity, zones)\n",
    "    return features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Processing Blocks and Weeks\n",
    "\n",
    "We now define functions that group activities into blocks (e.g. 3-month periods)\n",
    "and further split them into weeks. For simplicity, the notebook version assumes that\n",
    "the activities are already sorted in chronological order."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_weeks(activities, week_duration=7):\n",
    "    \"\"\"\n",
    "    Split activities (assumed sorted by start_date ascending) into weekly lists.\n",
    "    \"\"\"\n",
    "    if not activities:\n",
    "        return []\n",
    "    weeks = []\n",
    "    # Convert start_date strings to datetime objects\n",
    "    for act in activities:\n",
    "        act[\"start_date_dt\"] = datetime.datetime.strptime(act[\"start_date\"][:10], '%Y-%m-%d')\n",
    "    # Sort by date (oldest first)\n",
    "    activities.sort(key=lambda x: x[\"start_date_dt\"])\n",
    "    start_date = activities[0][\"start_date_dt\"]\n",
    "    current_week = []\n",
    "    for act in activities:\n",
    "        if (act[\"start_date_dt\"] - start_date).days < week_duration:\n",
    "            current_week.append(act)\n",
    "        else:\n",
    "            weeks.append(current_week)\n",
    "            current_week = [act]\n",
    "            start_date = act[\"start_date_dt\"]\n",
    "    if current_week:\n",
    "        weeks.append(current_week)\n",
    "    return weeks\n",
    "\n",
    "def process_activity_block(activities, zones, hr_regressor=None):\n",
    "    \"\"\"\n",
    "    Process a block of activities:\n",
    "      - Extract features for each activity.\n",
    "      - Group activities into weeks and compute weekly aggregate metrics.\n",
    "    Returns:\n",
    "      - A DataFrame of activity-level features.\n",
    "      - A DataFrame of weekly features.\n",
    "    \"\"\"\n",
    "    features_list = []\n",
    "    for act in activities:\n",
    "        feat = extract_activity_features(act, zones, hr_regressor)\n",
    "        # Optionally add athlete or block info here\n",
    "        features_list.append(feat)\n",
    "    activities_df = pd.DataFrame(features_list)\n",
    "    \n",
    "    # Process weekly aggregates\n",
    "    weeks = get_weeks(activities)\n",
    "    weekly_features = []\n",
    "    for i, week in enumerate(weeks):\n",
    "        week_feats = {}\n",
    "        week_df = pd.DataFrame(week)\n",
    "        week_feats[\"week_num\"] = i\n",
    "        week_feats[\"total_activities\"] = len(week)\n",
    "        week_feats[\"total_distance\"] = week_df[\"distance\"].sum() if \"distance\" in week_df.columns else 0\n",
    "        week_feats[\"avg_elapsed_time\"] = week_df[\"elapsed_time\"].mean() if \"elapsed_time\" in week_df.columns else None\n",
    "        weekly_features.append(week_feats)\n",
    "    weeks_df = pd.DataFrame(weekly_features)\n",
    "    return activities_df, weeks_df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Main Transformation Function\n",
    "\n",
    "This function loads the athlete’s raw data (athlete metadata, zones, activities),\n",
    "builds the HR regressor, cleans the activities, and then processes the block and week features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_athlete_data(athlete_id, populate_all_from_files=True):\n",
    "    \"\"\"\n",
    "    Main pipeline to transform athlete data.\n",
    "    Expects files:\n",
    "      - athlete_{athlete_id}_athlete.json\n",
    "      - athlete_{athlete_id}_zones.json\n",
    "      - A directory with individual activity JSON files (named as {activity_id}.json)\n",
    "    Returns:\n",
    "      - A dictionary containing DataFrames for raw activities and weekly aggregates.\n",
    "    \"\"\"\n",
    "    athlete_file = os.path.join(DATA_DIR, f\"athlete_{athlete_id}_athlete.json\")\n",
    "    zones_file = os.path.join(DATA_DIR, f\"athlete_{athlete_id}_zones.json\")\n",
    "    activities_dir = os.path.join(DATA_DIR, str(athlete_id))\n",
    "    \n",
    "    athlete_data = load_json_file(athlete_file)\n",
    "    zones_data = load_json_file(zones_file)\n",
    "    \n",
    "    if athlete_data is None or zones_data is None:\n",
    "        logger.error(\"Athlete metadata or zones data could not be loaded.\")\n",
    "        return None\n",
    "    \n",
    "    # Combine zones into athlete_data under \"_Zones\"\n",
    "    athlete_data[\"_Zones\"] = zones_data\n",
    "    # For simplicity, we won’t use stats in this example.\n",
    "    \n",
    "    # Load all activity files from the athlete directory\n",
    "    all_activities = []\n",
    "    if os.path.exists(activities_dir):\n",
    "        for fname in os.listdir(activities_dir):\n",
    "            if fname.endswith(\".json\") and fname[:-5].isdigit():\n",
    "                activity = load_json_file(os.path.join(activities_dir, fname))\n",
    "                if activity:\n",
    "                    all_activities.append(activity)\n",
    "    else:\n",
    "        logger.error(f\"Activities directory {activities_dir} does not exist.\")\n",
    "    \n",
    "    logger.info(f\"Loaded {len(all_activities)} activities for athlete {athlete_id}\")\n",
    "    # Filter out activities with errors\n",
    "    clean_activities = get_error_free_activities(all_activities)\n",
    "    \n",
    "    # Get HR zones\n",
    "    zones = get_athlete_zones(athlete_data)\n",
    "    logger.info(f\"Using HR zones: {zones}\")\n",
    "    \n",
    "    # Build HR regressor for run activities\n",
    "    hr_regressor, reg_data = build_pace_to_hr_regressor(clean_activities, athlete_id, zones)\n",
    "    if hr_regressor:\n",
    "        logger.info(\"HR regressor built successfully.\")\n",
    "    else:\n",
    "        logger.info(\"Insufficient data for HR regressor; proceeding without it.\")\n",
    "    \n",
    "    # Process activities into features and weeks\n",
    "    activities_df, weeks_df = process_activity_block(clean_activities, zones, hr_regressor)\n",
    "    logger.info(\"Processed activities into feature DataFrames.\")\n",
    "    \n",
    "    return {\n",
    "        \"athlete_data\": athlete_data,\n",
    "        \"activities_df\": activities_df,\n",
    "        \"weeks_df\": weeks_df\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing the Pipeline\n",
    "\n",
    "Let's run the pipeline for a given athlete (make sure the required JSON files exist in the correct folders)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "athlete_id = 64383208  # example athlete id\n",
    "result = transform_athlete_data(athlete_id)\n",
    "\n",
    "if result:\n",
    "    print(\"Athlete metadata:\")\n",
    "    print(result[\"athlete_data\"])\n",
    "    print(\"\\nActivity-level features:\")\n",
    "    display(result[\"activities_df\"].head())\n",
    "    print(\"\\nWeekly aggregates:\")\n",
    "    display(result[\"weeks_df\"].head())\n",
    "else:\n",
    "    print(\"Error processing athlete data.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PEUT ETRE REFAIRE CAR IL A PAS TOUT PRIS EN COMPTE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
